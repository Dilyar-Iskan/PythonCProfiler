{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MT_Lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dilyar-Iskan/PythonCProfiler/blob/master/MTLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFthftyA-wsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "import io\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDdy44lN-xu6",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation Lab: todo\n",
        "We ended up switching to Google Colab to utilize them GPU's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh8FsOHITfhb",
        "colab_type": "text"
      },
      "source": [
        "**Important**\n",
        "* replace greedy search with beam search\n",
        "* rewrite BPE\n",
        "* speeding up train_step\n",
        "\n",
        "**If we have enough time**\n",
        "* add dropout layers\n",
        "* replace GRU with LSTM (apparently LSTM perform slightly better)\n",
        "* add early stopping (as in checkpoint evaluation)\n",
        "* we seem to delete the dot sometimes somehow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBq-8tyXAEIg",
        "colab_type": "text"
      },
      "source": [
        "# Configuration\n",
        "We used to have a config file but that's really inconvenient with google colab. \n",
        "If needed we would just replace everything below with configparser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HE9jxWMZJ0E",
        "colab_type": "text"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsF1_8bATiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_main_dir = \"/content/drive/My Drive/Colab Notebooks/MT_Lab\"\n",
        "\n",
        "path_src_train = path_main_dir + \"/multi30k.de\"\n",
        "path_tar_train = path_main_dir + \"/multi30k.en\"\n",
        "path_src_val = path_main_dir + \"/multi30k.dev.de\"\n",
        "path_tar_val = path_main_dir + \"/multi30k.dev.en\"\n",
        "\n",
        "load_bpe_operations = False\n",
        "bpe_operations = 7500\n",
        "\n",
        "limit_training_data = 10000\n",
        "batch_size = 256\n",
        "buffer_size = limit_training_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83Z20Y2cZLUM",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FwazNdPZJNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dimension = 256\n",
        "encoder_units = 128\n",
        "decoder_units = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8TtfFUbZq4r",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0Y_K4Nln63D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "\n",
        "starting_epoch = 5\n",
        "final_epoch = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2quFpIAtGuC",
        "colab_type": "text"
      },
      "source": [
        "Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iksEVl-tJhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beam_width = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH967Pqi-4Jd",
        "colab_type": "text"
      },
      "source": [
        "#Loading Data & Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CKfkx1JCWce",
        "colab_type": "text"
      },
      "source": [
        "### Byte Pair Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Y19xcSCjdu",
        "colab_type": "text"
      },
      "source": [
        "get merge operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkM8L-ohCVzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_merge_operations(text, load_bpe_operations):\n",
        "  if load_bpe_operations:\n",
        "    pass\n",
        "    # load\n",
        "  else:\n",
        "    pass\n",
        "    # train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OsmnrTPCvLb",
        "colab_type": "text"
      },
      "source": [
        "bpe encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0ks_PRC6IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bpe_encode(sentence, merge_opration):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_0kWR9C6iV",
        "colab_type": "text"
      },
      "source": [
        "bpe decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g57Vjz3C7Nk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bpe_decode(sentence):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3PAfOopLGso",
        "colab_type": "text"
      },
      "source": [
        "###Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tt429vZB7iP",
        "colab_type": "text"
      },
      "source": [
        "Load the Data from the Drive.\n",
        "As of rn we do not use BPE because of irregularities in our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd-yd2p1B6SH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(path, sort = False):\n",
        "  input = io.open(path, encoding='UTF-8').read().split('\\n')\n",
        "  # todo bpe encode the sentence here\n",
        "  data = [\"<s> \" + sentence + \" </s>\" for sentence in input]\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an1FmwCbE8VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_train_txt = load_data(path_src_train)\n",
        "tar_train_txt = load_data(path_tar_train)\n",
        "src_val_txt = load_data(path_src_val)\n",
        "tar_val_txt = load_data(path_tar_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fo3fVO1XTN6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f5724ce-42b3-423d-ad8d-e8700afadbf6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N89zJfPTrfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sort_corpus(src_text, tar_text):\n",
        "  c = zip(src_text, tar_text)\n",
        "  c = sorted(c, key = lambda x: len(x[0])) \n",
        "  return zip(*c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo881oSSWKMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_train_txt, tar_train_txt = sort_corpus(src_train_txt, tar_train_txt)\n",
        "src_train_txt = src_train_txt[:10000]\n",
        "tar_train_txt = tar_train_txt[:10000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ilG2mEsFqtX",
        "colab_type": "text"
      },
      "source": [
        "Last element of each set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0sxNS0kFo7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8173e202-bf44-430d-f8ea-7e63766983c3"
      },
      "source": [
        "pos = 6\n",
        "print(src_train_txt[pos])\n",
        "print(src_val_txt[pos])\n",
        "print(tar_train_txt[pos])\n",
        "print(tar_val_txt[pos])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> ein windhund rennt . </s>\n",
            "<s> ein brauner hund rennt dem schwarzen hund hinterher . </s>\n",
            "<s> a grayhound dog is sprinting . </s>\n",
            "<s> a brown dog is running after the black dog . </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpTwKPtmGgMm",
        "colab_type": "text"
      },
      "source": [
        "###Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xJPPa3Gw8T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_token_dict(text):\n",
        "    dict = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    dict.fit_on_texts(text)\n",
        "    return dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1bKKkDnGycf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokenized_tensor(source_text, target_text, src_dict, tar_dict):\n",
        "    tokenized_source = src_dict.texts_to_sequences(source_text)\n",
        "    # post stands for padding after the sequence\n",
        "    tokenized_source = tf.keras.preprocessing.sequence.pad_sequences(tokenized_source, padding='post')\n",
        "    tokenized_target = tar_dict.texts_to_sequences(target_text)\n",
        "    tokenized_target = tf.keras.preprocessing.sequence.pad_sequences(tokenized_target, padding='post')\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((tokenized_source, tokenized_target))\n",
        "    return tokenized_source, tokenized_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rluhIA6uHNPu",
        "colab_type": "text"
      },
      "source": [
        "getting the dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFEP0LZ_G51s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_dict = create_token_dict(src_train_txt)\n",
        "tar_dict = create_token_dict(tar_train_txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9NBz5BPHU2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src_train, tar_train = get_tokenized_tensor(src_train_txt, tar_train_txt, src_dict, tar_dict)\n",
        "\n",
        "max_src_len = src_train.shape[1]\n",
        "max_tar_len = tar_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTsu0h5aBtq",
        "colab_type": "text"
      },
      "source": [
        "length of training data and length of longest sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q9EmAgK_wRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "abcb04a0-5687-45fa-e70a-afd83aa0498b"
      },
      "source": [
        "print(len(src_train))\n",
        "print(len(tar_train))\n",
        "\n",
        "print(max_src_len)\n",
        "print(max_tar_len)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "10000\n",
            "16\n",
            "23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cGLZZXeOUAS",
        "colab_type": "text"
      },
      "source": [
        "length of vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ux8eDm9OBW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_length_src = len(src_dict.index_word) + 1\n",
        "vocab_length_tar = len(tar_dict.index_word) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsoE63KInW_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22989ba7-c0fb-468b-f8ea-3ed24cee689d"
      },
      "source": [
        "print(vocab_length_src)\n",
        "print(vocab_length_tar)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6909\n",
            "4859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izj5TFzuICjJ",
        "colab_type": "text"
      },
      "source": [
        "printing first element of sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl0iH43MIGAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e1101fd9-5449-4351-942a-4531f5406ab2"
      },
      "source": [
        "print(src_train[pos])\n",
        "print(src_dict.sequences_to_texts([src_train[pos]]))\n",
        "print(tar_train[pos])\n",
        "print(tar_dict.sequences_to_texts([tar_train[pos]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1    4 1088   71    3    2    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "['<s> ein windhund rennt . </s>']\n",
            "[   2    1 2637   18    7 2638    4    3    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0]\n",
            "['<s> a grayhound dog is sprinting . </s>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXaAtdlRKkzQ",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blhmYwoJLiKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dataset():\n",
        "  return tf.data.Dataset.from_tensor_slices((src_train, tar_train)).shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBAaA8zc_gqK",
        "colab_type": "text"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esivMiIjB5Px",
        "colab_type": "text"
      },
      "source": [
        "### Encoder\n",
        "we're using GRU's rn but we'll have to switch them out for LSTM\n",
        "\n",
        "***todo:*** reverse input (go backwards -> look at tf documentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvLjZCBSNQrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.embedding_layer = tf.keras.layers.Embedding(vocab_length_src, embedding_dimension)\n",
        "      self.gru = tf.keras.layers.GRU(encoder_units, return_sequences=True, return_state=True, go_backwards = False)  # will have to be replaced with LSTM (LSTM has 3 inputs not 2)\n",
        "\n",
        "  def call(self, x, last_hidden_state):\n",
        "    x = self.embedding_layer(x)\n",
        "    hidden_seguence, hidden_state = self.gru(x, initial_state = last_hidden_state)\n",
        "    return hidden_seguence, hidden_state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((batch_size, encoder_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIVvGbhCQnsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHHU59m1RTUP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb771a0f-c752-4729-b81a-6166f66f65ee"
      },
      "source": [
        "dataset = get_dataset()\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([256, 16]), TensorShape([256, 23]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 919
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRH_-4j2RZ_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f8f937c7-ef3b-494b-f5e1-ac97966b61c7"
      },
      "source": [
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (256, 16, 128)\n",
            "Encoder Hidden state shape: (batch size, units) (256, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFPAPgZ2jDzS",
        "colab_type": "text"
      },
      "source": [
        "### Attention Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef_dgQPijMQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(Attention, self).__init__()\n",
        "    self.A = tf.keras.layers.Dense(decoder_units)\n",
        "    self.B = tf.keras.layers.Dense(encoder_units)\n",
        "    self.v = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, encoder_output, decoder_input):\n",
        "    # add sequence dimension to decoder input\n",
        "    decoder_input = tf.expand_dims(decoder_input, 1)\n",
        "    # sigma(Ax + Bs)\n",
        "    attention_weights = tf.nn.tanh(self.A(encoder_output) + self.B(decoder_input))\n",
        "    attention_weights = self.v(attention_weights)\n",
        "    attention_weights = tf.nn.softmax(attention_weights, axis=1)\n",
        "    context_vector = attention_weights * encoder_output\n",
        "    return tf.reduce_sum(context_vector, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNetF8tAqakY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "274f8157-f910-4538-8b51-2fd59ee32bd4"
      },
      "source": [
        "attention_layer = Attention()\n",
        "attention_result = attention_layer(sample_output, sample_hidden)\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (256, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVwVMMgRpWf",
        "colab_type": "text"
      },
      "source": [
        "### Decoder\n",
        "Just as with the encoder the decoder uses GRU instead of LSTM rn, which has to be changed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpuvKsi8R4S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_length_tar, embedding_dimension)\n",
        "    self.gru = tf.keras.layers.GRU(decoder_units, return_sequences=True, return_state=True)  # has to be replaced with LSTM\n",
        "    self.attention_layer = Attention()\n",
        "    self.output_layer = tf.keras.layers.Dense(vocab_length_tar)\n",
        "\n",
        "  def call(self, x, last_hidden_state, encoder_output):\n",
        "    x = self.embedding_layer(x)\n",
        "    hidden_sequence, hidden_state = self.gru(x, initial_state = last_hidden_state)\n",
        "    c = self.attention_layer(encoder_output, hidden_state)\n",
        "    concat = tf.concat([c, hidden_state], axis = 1)\n",
        "    output = self.output_layer(concat)\n",
        "    return output, hidden_state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((batch_size, decoder_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xaDWCnSZAZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipqvh9LvaKvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a3b3c8b8-4444-47ce-c292-da40b2d7b61a"
      },
      "source": [
        "sample_decoder_output, sample_decoder_hidden = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder pred shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
        "print ('Decoder hidden shape: (batch_size, vocab size) {}'.format(sample_decoder_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder pred shape: (batch_size, vocab size) (256, 4859)\n",
            "Decoder hidden shape: (batch_size, vocab size) (256, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6JD0f0h_0WS",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O25JVfmR16qq",
        "colab_type": "text"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8NUn50M1_ZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_prefix = os.path.join(path_main_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RXZ2dbo22He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def restore_checkpoint():\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(path_main_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-fZvA8Bnyg3",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtaR-a3-qWO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function2(lbl, pred):\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(lbl, pred)\n",
        "    eos = tf.math.not_equal(lbl, 0)     # eos is false for all non tokens\n",
        "    # potential problem: sentences that are too long are not being penalized\n",
        "    loss = tf.boolean_mask(loss, eos)\n",
        "    mean = tf.reduce_mean(loss)\n",
        "    return mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT3IEYyP4tQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4eRmGh8RoEF",
        "colab_type": "text"
      },
      "source": [
        "### Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VppO0BgoURgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(src, tar, encoder_hidden, decoder_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoder_hidden_sequence, encoder_last_hidden_state = encoder(src, encoder_hidden)\n",
        "    decoder_input = tf.expand_dims([tar_dict.word_index['<s>']] * batch_size, 1)\n",
        "    \n",
        "    for i in range(1, max_tar_len):\n",
        "      pred, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hidden_sequence)\n",
        "      loss += loss_function(tar[:, i], pred)\n",
        "      decoder_input = tf.expand_dims(tar[:, i], 1)\n",
        "      \n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "  return loss / max_tar_len\n",
        "  # max tar len is not a tensor! -> potential slowdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoneqcGk9vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_step(example_input_batch, example_target_batch, encoder.initialize_hidden_state(), decoder.initialize_hidden_state())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvBPJM37tdNu",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sY0JCQj-Afj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_batches = len(src_train) // batch_size\n",
        "steps_per_epoch = total_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LINYTWfwCRnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50dffca6-a4ee-40cd-8c00-e4747c511117"
      },
      "source": [
        "print(total_batches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehoc6bhowZc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(starting_epoch=0, final_epoch=10):\n",
        "  if starting_epoch > 0:\n",
        "    restore_checkpoint()\n",
        "\n",
        "  for epoch in range(starting_epoch, final_epoch):\n",
        "    s = time.time()\n",
        "    total_loss = 0\n",
        "    # initialized here instead of train_step for performance reasons\n",
        "    encoder_initial_hidden_state = encoder.initialize_hidden_state()\n",
        "    decoder_initial_hidden_state = decoder.initialize_hidden_state()\n",
        "\n",
        "    for i, (src, tar) in enumerate(dataset.take(steps_per_epoch)):\n",
        "      batch_loss = train_step(src, tar, encoder_initial_hidden_state, decoder_initial_hidden_state)\n",
        "      total_loss += batch_loss\n",
        "      if i % 50 == 0: \n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, i, batch_loss.numpy()))\n",
        "      \n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    print('\\nEpoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n\\n'.format(time.time() - s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYWFcKcY1QIS",
        "colab_type": "text"
      },
      "source": [
        "*run this cell to train model:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q5SEbmhHgcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "263df996-3b43-4dce-c203-d05634ec5411"
      },
      "source": [
        "print(starting_epoch)\n",
        "print(final_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKCflejDzYOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model(starting_epoch = starting_epoch, final_epoch = final_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-K_21z2_20F",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG1a9tHmpAqG",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAB-QxNo6Tfx",
        "colab_type": "text"
      },
      "source": [
        "evaluate gets a list with a single tokenized sentence in it as input\n",
        "\n",
        "***todo***: has to be changed to matrix input\n",
        "\n",
        "***todo***: convert greedy search to beam search\n",
        "\n",
        "***todo***: convert from eager to tf function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMUEf5H_6Bl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  src = tf.convert_to_tensor(sentence)\n",
        "\n",
        "  encoder_initial_hidden = [tf.zeros((1, encoder_units))]\n",
        "  encoder_hidden_sequence, encoder_last_hidden = encoder(src, encoder_initial_hidden)\n",
        "\n",
        "  decoder_hidden = [tf.zeros((1, decoder_units))]\n",
        "  decoder_input = tf.expand_dims([tar_dict.word_index['<s>']], 0)\n",
        "  \n",
        "  pred = tf.zeros(vocab_length_tar)\n",
        "  \n",
        "  prob = 0\n",
        "\n",
        "  res = []\n",
        "\n",
        "  for i in range(max_tar_len):\n",
        "    pred, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hidden_sequence)\n",
        "    pred = tf.argmax(pred[0]).numpy()\n",
        "    res += [pred]\n",
        "    decoder_input = tf.expand_dims([pred], 0)\n",
        "\n",
        "    # stop unrolling\n",
        "    if tar_dict.index_word[pred] == '</s>':\n",
        "      return res\n",
        "\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jHPOqafM4f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_beam_search(sentence):\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91tptW3bvQkw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we-hNZZaBCVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cf8476cb-0760-416b-b9ee-9dcb04626706"
      },
      "source": [
        "example_sentence = \"<s> personen stehen vor dem café und sind dabei , einzutreten . </s>\"\n",
        "example_sentence_tokenized = src_dict.texts_to_sequences([example_sentence])\n",
        "print(example_sentence_tokenized)\n",
        "evaluate(example_sentence_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 42, 50, 26, 30, 451, 13, 124, 356, 21, 6907, 3, 2]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[16, 28, 47, 10, 16, 5, 16, 93, 14, 113, 4, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 953
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVJdfOtFnrSY",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ioKYfj6g4D",
        "colab_type": "text"
      },
      "source": [
        "evaluate_list gets a list of non-tokenized sentences as input and returns a list of non-tokenized translations as output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQXXphE4AeQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_list(text):\n",
        "  return [evaluate(src_dict.texts_to_sequences([sentence])) for sentence in text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvdI-ZZHBDU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b660c2f5-0c9c-4899-9325-c8f652786f97"
      },
      "source": [
        "example_text = [example_sentence]\n",
        "print(example_text)\n",
        "evaluate_list(example_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s> personen stehen vor dem café und sind dabei , einzutreten . </s>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[16, 28, 47, 10, 16, 5, 16, 93, 14, 113, 4, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 955
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Am0nyOftFD7k",
        "colab_type": "text"
      },
      "source": [
        "### Translate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiC8WYP7s6pj",
        "colab_type": "text"
      },
      "source": [
        "translate_sentence for testing purposses only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf5WMters542",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(sentence):\n",
        "  # bpe\n",
        "  sentence = \"<s> \" + sentence + \" </s>\"\n",
        "  t = evaluate(src_dict.texts_to_sequences([example_sentence]))\n",
        "  t = tar_dict.sequences_to_texts([t])[0]\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm0XKbJotC9H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "da22990a-a3ef-4c91-fdda-c7e5e45f0c97"
      },
      "source": [
        "example_sentence = \"personen stehen vor dem café und sind dabei , einzutreten .\"\n",
        "print(src_dict.texts_to_sequences([example_sentence]))\n",
        "translate_sentence(example_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[42, 50, 26, 30, 451, 13, 124, 356, 21, 6907, 3]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'people standing outside of a cafe about to go in the background . </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 968
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kADPDCcKEtlt",
        "colab_type": "text"
      },
      "source": [
        "translate gets list of non preprocessed, non-tokenized text as input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GaduSXRLkWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_start_token(text):\n",
        "  t = [\" \".join(s.split()[1:]) for s in text]\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6rFhfMYLlJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_end_token(text):\n",
        "  t = [\" \".join(s.split()[:-1]) for s in text]\n",
        "  return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWJ0LEBnLpOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(text):\n",
        "  t = [translate_sentence(sentence) for sentence in text]\n",
        "  return strip_end_token(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKx3ns95L01B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94826f14-4c1a-48d9-df1d-c0da0940c2b7"
      },
      "source": [
        "translate([example_sentence])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['people standing outside of a cafe about to go in the background .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 974
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3IkyraNSKnZ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLUhrS_YKA-8",
        "colab_type": "text"
      },
      "source": [
        "### BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwSJHIi7KtQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def n_gram(reference, hypothesis, n=4):\n",
        "    # returns the clipped count of n-grams in the hypothesis that match a n-gram in the reference\n",
        "    # can be used to count the total amount of n-grams in a String S by calling n_grams(S, S)\n",
        "    reference = reference.split()\n",
        "    hypothesis = hypothesis.split()\n",
        "    reference_n_grams = []\n",
        "    hypothesis_n_grams = []\n",
        "    ret = 0\n",
        "\n",
        "    # creating n-gram lists\n",
        "    for i in range(len(reference) - n + 1):\n",
        "        tmp = []\n",
        "        for j in range(n):\n",
        "            tmp.append(reference[i + j])\n",
        "        reference_n_grams.append(tmp)\n",
        "\n",
        "    for i in range(len(hypothesis) - n + 1):\n",
        "        tmp = []\n",
        "        for j in range(n):\n",
        "            tmp.append(hypothesis[i + j])\n",
        "        hypothesis_n_grams.append(tmp)\n",
        "\n",
        "    # clipped count\n",
        "    for n_gram in hypothesis_n_grams:\n",
        "        if n_gram in reference_n_grams:\n",
        "            ret += 1\n",
        "            reference_n_grams.remove(n_gram)  # the remove command in python removes the first instance that is found\n",
        "\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEmGOsmcKvdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modified_n_gram_precision(L, n=4):  # each element in L has form (reference, hypothesis)\n",
        "    counter = 0\n",
        "    denominator = 0\n",
        "    for pair in L:\n",
        "        counter += n_gram(pair[0], pair[1], n)      # clipped count of matching n-grams\n",
        "        denominator += n_gram(pair[1], pair[1], n)  # total amount of n-grams in the hypothesis\n",
        "    if denominator != 0:\n",
        "        return counter/denominator\n",
        "    else:\n",
        "        return -1\n",
        "        # this should never be returned as long as there is at least one sentence in the hypothesis that is n long"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGY0gZyrK53S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def brevity_penalty(c, r):\n",
        "    \"\"\"     c := hypothesis length\n",
        "            r := reference length\n",
        "    \"\"\"\n",
        "    if c > r:\n",
        "        return 1\n",
        "    else:\n",
        "        return math.exp(1-r/c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBcVOncPLofk",
        "colab_type": "text"
      },
      "source": [
        "### Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ndEusoK-Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bleu_score(hyp, ref, n=4):  # each element in L has form (reference, hypothesis)\n",
        "    L = []\n",
        "    for t, d in zip(ref, hyp):\n",
        "        L.append([t, d])\n",
        "    c = 0\n",
        "    r = 0\n",
        "    s = 0\n",
        "    for pair in L:\n",
        "        c += len(pair[1].split())\n",
        "        r += len(pair[0].split())\n",
        "    for i in range(1, n + 1):\n",
        "        tmp = modified_n_gram_precision(L, i)\n",
        "        if tmp != 0:    # the lecture didn't define what to do in this case, so I just set it to 0\n",
        "            s += (1/n) * math.log(tmp)\n",
        "    return brevity_penalty(c, r) * math.exp(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o2gSfcOSUxL",
        "colab_type": "text"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yYKZLp4MPHh",
        "colab_type": "text"
      },
      "source": [
        "Translate text calculate its BLEU score and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adPNWjABSHdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "limit_val = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3Ha86DzXeXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_and_save(src, ref):\n",
        "  hyp = translate(src)\n",
        "\n",
        "  ref = strip_start_token(ref)\n",
        "  ref = strip_end_token(ref)\n",
        "\n",
        "  bs = bleu_score(hyp, ref)\n",
        "  print(\"bleu score = \" + bs)\n",
        "  # todo save\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tBlWMK3Ryfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = src_val_txt[:limit_val]\n",
        "src = strip_start_token(ref)\n",
        "s = strip_end_token(ref)\n",
        "\n",
        "\n",
        "hyp = translate(src_val_txt[:limit_val])\n",
        "hyp = strip_end_token(hyp)\n",
        "\n",
        "ref = tar_val_txt[:limit_val]\n",
        "ref = strip_start_token(ref)\n",
        "ref = strip_end_token(ref)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn5D_gM-bql-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(src_val_txt[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pps_6P6WA4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyp[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCg4DeZ_VTCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ref[:10]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}